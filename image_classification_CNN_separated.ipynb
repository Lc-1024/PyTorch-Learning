{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个文件是为了将双层的CNN模型分离成两个部分，以此在client和server两端分开计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import time\n",
    "import struct\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "# 固定初始化种子\n",
    "SEED = 24\n",
    "torch.manual_seed(SEED)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\".data\", train=True, download=True,\n",
    "           transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               # Normalize输入为两个tuple，output=(input-mean)/std\n",
    "               transforms.Normalize((0.13066,), (0.30811,)) # (x,)输出为一维tuple\n",
    "           ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, \n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\".data\", train=False, download=True,\n",
    "           transform=transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               # Normalize输入为两个tuple，output=(input-mean)/std\n",
    "               transforms.Normalize((0.13066,), (0.30811,)) # (x,)输出为一维tuple\n",
    "           ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, \n",
    "    num_workers=0, pin_memory=False\n",
    ")\n",
    "print(\"Dataloader successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是正常的模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 channel -> 20 channels\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) # 28 * 28 -> (28+1-5) = 24 * 24\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size * 1 * 28 * 28\n",
    "        x = F.relu(self.conv1(x)) # batch_size * 20 * 24 * 24\n",
    "        x = F.max_pool2d(x,2,2) # batch_size * 20 * 12 * 12\n",
    "        x = F.relu(self.conv2(x)) # batch_size * 50 * 8 * 8\n",
    "        x = F.max_pool2d(x,2,2) # batch_size * 50 * 4 *4 \n",
    "        x = x.view(-1, 4*4*50) # batch_size * (50*4*4) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x= self.fc2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1) # log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最简单粗暴的分离\n",
    "# 将第一层的输出作为clientNet的输出\n",
    "# 同时还是serverNet的输入\n",
    "class clientNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(clientNet, self).__init__()\n",
    "        # 1 channel -> 20 channels\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) # 28 * 28 -> (28+1-5) = 24 * 24\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size * 1 * 28 * 28\n",
    "        x = F.relu(self.conv1(x)) # batch_size * 20 * 24 * 24\n",
    "        x = F.max_pool2d(x,2,2) # batch_size * 20 * 12 * 12\n",
    "        return x\n",
    "    \n",
    "class serverNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(serverNet, self).__init__()\n",
    "        # 1 channel -> 20 channels\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) # 28 * 28 -> (28+1-5) = 24 * 24\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv2(x)) # batch_size * 50 * 8 * 8\n",
    "        x = F.max_pool2d(x,2,2) # batch_size * 50 * 4 *4 \n",
    "        x = x.view(-1, 4*4*50) # batch_size * (50*4*4) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x= self.fc2(x)\n",
    "        # return x\n",
    "        return F.log_softmax(x, dim=1) # log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这是正常的训练过程\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration:    0, Loss: 2.349389\n",
      "Train Epoch: 0, iteration:  100, Loss: 0.464970\n",
      "Train Epoch: 0, iteration:  200, Loss: 0.241684\n",
      "Train Epoch: 0, iteration:  300, Loss: 0.121889\n",
      "Train Epoch: 0, iteration:  400, Loss: 0.140317\n",
      "Train Epoch: 0, iteration:  500, Loss: 0.109634\n",
      "Train Epoch: 0, iteration:  600, Loss: 0.251497\n",
      "Train Epoch: 0, iteration:  700, Loss: 0.255145\n",
      "Train Epoch: 0, iteration:  800, Loss: 0.089439\n",
      "Train Epoch: 0, iteration:  900, Loss: 0.067693\n",
      "Train Epoch: 0, iteration: 1000, Loss: 0.051744\n",
      "Train Epoch: 0, iteration: 1100, Loss: 0.079346\n",
      "Train Epoch: 0, iteration: 1200, Loss: 0.036770\n",
      "Train Epoch: 0, iteration: 1300, Loss: 0.047175\n",
      "Train Epoch: 0, iteration: 1400, Loss: 0.358099\n",
      "Train Epoch: 0, iteration: 1500, Loss: 0.128052\n",
      "Train Epoch: 0, iteration: 1600, Loss: 0.131123\n",
      "Train Epoch: 0, iteration: 1700, Loss: 0.133915\n",
      "Train Epoch: 0, iteration: 1800, Loss: 0.112847\n",
      "Train Epoch: 1, iteration:    0, Loss: 0.091838\n",
      "Train Epoch: 1, iteration:  100, Loss: 0.077647\n",
      "Train Epoch: 1, iteration:  200, Loss: 0.155822\n",
      "Train Epoch: 1, iteration:  300, Loss: 0.199222\n",
      "Train Epoch: 1, iteration:  400, Loss: 0.060684\n",
      "Train Epoch: 1, iteration:  500, Loss: 0.051593\n",
      "Train Epoch: 1, iteration:  600, Loss: 0.013293\n",
      "Train Epoch: 1, iteration:  700, Loss: 0.026547\n",
      "Train Epoch: 1, iteration:  800, Loss: 0.012808\n",
      "Train Epoch: 1, iteration:  900, Loss: 0.028117\n",
      "Train Epoch: 1, iteration: 1000, Loss: 0.040759\n",
      "Train Epoch: 1, iteration: 1100, Loss: 0.011237\n",
      "Train Epoch: 1, iteration: 1200, Loss: 0.050422\n",
      "Train Epoch: 1, iteration: 1300, Loss: 0.022139\n",
      "Train Epoch: 1, iteration: 1400, Loss: 0.066629\n",
      "Train Epoch: 1, iteration: 1500, Loss: 0.010958\n",
      "Train Epoch: 1, iteration: 1600, Loss: 0.026679\n",
      "Train Epoch: 1, iteration: 1700, Loss: 0.005095\n",
      "Train Epoch: 1, iteration: 1800, Loss: 0.011840\n",
      "cost time: 35.821333\n"
     ]
    }
   ],
   "source": [
    "start_1 = time.time()\n",
    "for epoch in range(2):\n",
    "    for i, (data, target) in enumerate(train_dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        pred = model(data) # batch_size * 10\n",
    "        loss = F.nll_loss(pred, target) \n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Train Epoch: {}, iteration: {:>4d}, Loss: {:.6f}\".format(\n",
    "                epoch, i, loss.item()))\n",
    "print(\"cost time: {:.6f}\".format(time.time()-start_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "serverNet(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这是分开后的model做的训练\n",
    "client_model = clientNet().to(device)\n",
    "server_model = serverNet().to(device)\n",
    "client_optimizer = torch.optim.SGD(client_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "server_optimizer = torch.optim.SGD(server_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "client_model.train()\n",
    "server_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration:    0, Loss: 2.287267\n",
      "Train Epoch: 0, iteration:  100, Loss: 0.625813\n",
      "Train Epoch: 0, iteration:  200, Loss: 0.480779\n",
      "Train Epoch: 0, iteration:  300, Loss: 0.093468\n",
      "Train Epoch: 0, iteration:  400, Loss: 0.237676\n",
      "Train Epoch: 0, iteration:  500, Loss: 0.046658\n",
      "Train Epoch: 0, iteration:  600, Loss: 0.085467\n",
      "Train Epoch: 0, iteration:  700, Loss: 0.272831\n",
      "Train Epoch: 0, iteration:  800, Loss: 0.025104\n",
      "Train Epoch: 0, iteration:  900, Loss: 0.140939\n",
      "Train Epoch: 0, iteration: 1000, Loss: 0.051098\n",
      "Train Epoch: 0, iteration: 1100, Loss: 0.179332\n",
      "Train Epoch: 0, iteration: 1200, Loss: 0.267376\n",
      "Train Epoch: 0, iteration: 1300, Loss: 0.089552\n",
      "Train Epoch: 0, iteration: 1400, Loss: 0.061261\n",
      "Train Epoch: 0, iteration: 1500, Loss: 0.029585\n",
      "Train Epoch: 0, iteration: 1600, Loss: 0.168747\n",
      "Train Epoch: 0, iteration: 1700, Loss: 0.042958\n",
      "Train Epoch: 0, iteration: 1800, Loss: 0.059305\n",
      "Train Epoch: 1, iteration:    0, Loss: 0.026790\n",
      "Train Epoch: 1, iteration:  100, Loss: 0.007488\n",
      "Train Epoch: 1, iteration:  200, Loss: 0.096138\n",
      "Train Epoch: 1, iteration:  300, Loss: 0.059958\n",
      "Train Epoch: 1, iteration:  400, Loss: 0.031737\n",
      "Train Epoch: 1, iteration:  500, Loss: 0.161505\n",
      "Train Epoch: 1, iteration:  600, Loss: 0.027842\n",
      "Train Epoch: 1, iteration:  700, Loss: 0.061547\n",
      "Train Epoch: 1, iteration:  800, Loss: 0.079818\n",
      "Train Epoch: 1, iteration:  900, Loss: 0.019546\n",
      "Train Epoch: 1, iteration: 1000, Loss: 0.007823\n",
      "Train Epoch: 1, iteration: 1100, Loss: 0.100319\n",
      "Train Epoch: 1, iteration: 1200, Loss: 0.010095\n",
      "Train Epoch: 1, iteration: 1300, Loss: 0.013267\n",
      "Train Epoch: 1, iteration: 1400, Loss: 0.013870\n",
      "Train Epoch: 1, iteration: 1500, Loss: 0.106447\n",
      "Train Epoch: 1, iteration: 1600, Loss: 0.012393\n",
      "Train Epoch: 1, iteration: 1700, Loss: 0.001206\n",
      "Train Epoch: 1, iteration: 1800, Loss: 0.005082\n",
      "cost time: 39.393186\n"
     ]
    }
   ],
   "source": [
    "# 这是分开后的训练的过程\n",
    "start_2 = time.time()\n",
    "for epoch in range(2):\n",
    "    for i, (data, target) in enumerate(train_dataloader):\n",
    "        # 按顺序通过client_model和server_model，得到pred为预测的结果，计算loss\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = client_model(data)\n",
    "        pred = server_model(output)\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        # 然后先进行server的backward和optimize，然后获得output的grad\n",
    "        server_optimizer.zero_grad()\n",
    "        output.retain_grad() # 必须要retain output的grad，才能获得grad\n",
    "        loss.backward()\n",
    "        server_optimizer.step()\n",
    "        grad = output.grad.data\n",
    "        # 最后再利用output的grad对client进行backward和optimize\n",
    "        client_optimizer.zero_grad()\n",
    "        output = client_model(data) # output需要先利用之前的input计算获得graph\n",
    "        output.backward(grad)       # 才能利用grad进行backward\n",
    "        client_optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"Train Epoch: {}, iteration: {:>4d}, Loss: {:.6f}\".format(\n",
    "                epoch, i, loss.item()))\n",
    "print(\"cost time: {:.6f}\".format(time.time()-start_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04174569380283356, Accuracy: 98.61\n"
     ]
    }
   ],
   "source": [
    "# 正常模型在测试集上的准确率\n",
    "model.eval()\n",
    "total_loss = 0.\n",
    "correct = 0.\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(test_dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data) # batch_size * 10\n",
    "        total_loss += F.nll_loss(output, target, reduction=\"sum\").item() \n",
    "        pred = output.argmax(dim=1) # batch_size * 1\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "total_loss /= len(test_dataloader.dataset)\n",
    "acc = correct/len(test_dataloader.dataset) * 100.\n",
    "print(\"Test loss: {}, Accuracy: {}\".format(total_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.044531891798973086, Accuracy: 98.6\n"
     ]
    }
   ],
   "source": [
    "# 分离后模型在测试集上的准确率\n",
    "client_model.eval()\n",
    "server_model.eval()\n",
    "total_loss = 0.\n",
    "correct = 0.\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(test_dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = server_model(client_model(data)) # batch_size * 10\n",
    "        total_loss += F.nll_loss(output, target, reduction=\"sum\").item() \n",
    "        pred = output.argmax(dim=1) # batch_size * 1\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "total_loss /= len(test_dataloader.dataset)\n",
    "acc = correct/len(test_dataloader.dataset) * 100.\n",
    "print(\"Test loss: {}, Accuracy: {}\".format(total_loss, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
